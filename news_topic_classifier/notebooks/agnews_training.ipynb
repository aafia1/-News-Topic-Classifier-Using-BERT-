{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eacc58d",
   "metadata": {},
   "source": [
    "# ðŸ“˜ AG News Topic Classification using BERT\n",
    "\n",
    "This notebook fine-tunes a BERT model on the AG News dataset for topic classification.\n",
    "We will:\n",
    "- Load dataset\n",
    "- Preprocess and tokenize\n",
    "- Train BERT\n",
    "- Evaluate\n",
    "- Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6a351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anacondaaa\\lib\\site-packages (2.1.1)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/eb/62/eb8157afb21bd229c864521c1ab4fa8e9b4f1b06bafdd8c4668a7a31b5dd/datasets-4.0.0-py3-none-any.whl.metadata\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\anacondaaa\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aafia\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: boto3 in d:\\anacondaaa\\lib\\site-packages (from transformers) (1.24.28)\n",
      "Requirement already satisfied: requests in d:\\anacondaaa\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tqdm in d:\\anacondaaa\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex in d:\\anacondaaa\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\aafia\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.2.0)\n",
      "Requirement already satisfied: sacremoses in d:\\anacondaaa\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in d:\\anacondaaa\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=15.0.0 from https://files.pythonhosted.org/packages/6e/0b/77ea0600009842b30ceebc3337639a7380cd946061b620ac1a2f3cb541e2/pyarrow-21.0.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached pyarrow-21.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\anacondaaa\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\anacondaaa\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/52/1c/fa3b61c0cf03e1da4767213672efe186b1dfa4fc901a4a694fb184a513d1/xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Obtaining dependency information for multiprocess<0.70.17 from https://files.pythonhosted.org/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl.metadata\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<=2025.3.0,>=2023.1.0 from https://files.pythonhosted.org/packages/56/53/eb690efa8513166adef3e0669afd31e95ffde69fb3c52ec2ac7223ed6018/fsspec-2025.3.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\aafia\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.34.1)\n",
      "Requirement already satisfied: packaging in d:\\anacondaaa\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anacondaaa\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\aafia\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anacondaaa\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anacondaaa\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\anacondaaa\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aafia\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anacondaaa\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anacondaaa\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacondaaa\\lib\\site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacondaaa\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: colorama in d:\\anacondaaa\\lib\\site-packages (from tqdm->transformers) (0.4.6)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in d:\\anacondaaa\\lib\\site-packages (from boto3->transformers) (1.27.59)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\anacondaaa\\lib\\site-packages (from boto3->transformers) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in d:\\anacondaaa\\lib\\site-packages (from boto3->transformers) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anacondaaa\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anacondaaa\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anacondaaa\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six in d:\\anacondaaa\\lib\\site-packages (from sacremoses->transformers) (1.17.0)\n",
      "Requirement already satisfied: click in d:\\anacondaaa\\lib\\site-packages (from sacremoses->transformers) (8.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anacondaaa\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anacondaaa\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anacondaaa\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anacondaaa\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anacondaaa\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anacondaaa\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anacondaaa\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow-21.0.0-cp311-cp311-win_amd64.whl (26.2 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Installing collected packages: xxhash, pyarrow, multiprocess, fsspec, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "Successfully installed datasets-4.0.0 fsspec-2023.3.0 multiprocess-0.70.16 pyarrow-21.0.0 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.3.0 requires fsspec==2023.3.0, but you have fsspec 2025.3.0 which is incompatible.\n",
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BertForSequenceClassification' from 'transformers' (D:\\anacondaaa\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     BertTokenizer, \n\u001b[0;32m     10\u001b[0m     BertForSequenceClassification, \n\u001b[0;32m     11\u001b[0m     Trainer, \n\u001b[0;32m     12\u001b[0m     TrainingArguments, \n\u001b[0;32m     13\u001b[0m     DataCollatorWithPadding\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, f1_score\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BertForSequenceClassification' from 'transformers' (D:\\anacondaaa\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# 1. Install & Import Libraries\n",
    "# ====================================\n",
    "!pip install transformers datasets scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf7e9fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'transformers' has no attribute 'PreTrainedTokenizerBase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ====================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 2. Dataset Load\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ====================================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorld\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSports\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBusiness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSci/Tech\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_news\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect_cols\u001b[39m(batch):\n\u001b[0;32m      8\u001b[0m     texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1389\u001b[0m )\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1393\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   1394\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   1395\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1396\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1397\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1398\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1399\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1400\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1401\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1402\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1403\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1405\u001b[0m )\n\u001b[0;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\load.py:1180\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m builder_cls(\n\u001b[0;32m   1167\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1168\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\builder.py:612\u001b[0m, in \u001b[0;36mDatasetBuilder._use_legacy_cache_dir_if_possible\u001b[1;34m(self, dataset_module)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_use_legacy_cache_dir_if_possible\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetModule\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;66;03m# Check for the legacy cache directory template (datasets<3.0.0)\u001b[39;00m\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_relative_data_dir \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_legacy_cache2(dataset_module) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_legacy_cache() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    613\u001b[0m     )\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_cache_dir()\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_dir\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\builder.py:485\u001b[0m, in \u001b[0;36mDatasetBuilder._check_legacy_cache2\u001b[1;34m(self, dataset_module)\u001b[0m\n\u001b[0;32m    483\u001b[0m namespace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mobject(Pickler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_legacy_no_dict_keys_sorting\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 485\u001b[0m     config_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m Hasher\u001b[38;5;241m.\u001b[39mhash({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_files\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_files})\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m _PACKAGED_DATASETS_MODULES_2_15_HASHES\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    488\u001b[0m     dataset_module\u001b[38;5;241m.\u001b[39mbuilder_configs_parameters\u001b[38;5;241m.\u001b[39mmetadata_configs\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_configs_parameters\u001b[38;5;241m.\u001b[39mmetadata_configs\n\u001b[0;32m    490\u001b[0m ):\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\fingerprint.py:188\u001b[0m, in \u001b[0;36mHasher.hash\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhash\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mhash_bytes(dumps(value))\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\utils\\_dill.py:109\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pickle an object to a string.\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m file \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[1;32m--> 109\u001b[0m dump(obj, file)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\utils\\_dill.py:103\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file):\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pickle an object to a file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     Pickler(file, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdump(obj)\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\dill\\_dill.py:420\u001b[0m, in \u001b[0;36mPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj): \u001b[38;5;66;03m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[0;32m    419\u001b[0m     logger\u001b[38;5;241m.\u001b[39mtrace_setup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 420\u001b[0m     StockPickler\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[1;32m--> 487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(obj)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\utils\\_dill.py:70\u001b[0m, in \u001b[0;36mPickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj_type \u001b[38;5;129;01mis\u001b[39;00m FunctionType:\n\u001b[0;32m     69\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torchdynamo_orig_callable\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj)\n\u001b[1;32m---> 70\u001b[0m dill\u001b[38;5;241m.\u001b[39mPickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj, save_persistent_id\u001b[38;5;241m=\u001b[39msave_persistent_id)\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\dill\\_dill.py:414\u001b[0m, in \u001b[0;36mPickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    412\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: attribute lookup builtins.generator failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m GeneratorType\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(msg)\n\u001b[1;32m--> 414\u001b[0m StockPickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj, save_persistent_id)\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\dill\\_dill.py:1217\u001b[0m, in \u001b[0;36msave_module_dict\u001b[1;34m(pickler, obj)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dill(pickler, child\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m pickler\u001b[38;5;241m.\u001b[39m_session:\n\u001b[0;32m   1215\u001b[0m         \u001b[38;5;66;03m# we only care about session the first pass thru\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m         pickler\u001b[38;5;241m.\u001b[39m_first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m     StockPickler\u001b[38;5;241m.\u001b[39msave_dict(pickler, obj)\n\u001b[0;32m   1218\u001b[0m     logger\u001b[38;5;241m.\u001b[39mtrace(pickler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# D2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_setitems(obj\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\utils\\_dill.py:74\u001b[0m, in \u001b[0;36mPickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_batch_setitems\u001b[39m(\u001b[38;5;28mself\u001b[39m, items):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_no_dict_keys_sorting:\n\u001b[1;32m---> 74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_batch_setitems(items)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Ignore the order of keys in a dict\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;66;03m# Faster, but fails for unorderable elements\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\pickle.py:1003\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m   1001\u001b[0m     k, v \u001b[38;5;241m=\u001b[39m tmp[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1002\u001b[0m     save(k)\n\u001b[1;32m-> 1003\u001b[0m     save(v)\n\u001b[0;32m   1004\u001b[0m     write(SETITEM)\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# else tmp is empty, and we're done\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anacondaaa\\Lib\\site-packages\\datasets\\utils\\_dill.py:64\u001b[0m, in \u001b[0;36mPickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(obj_type, transformers\u001b[38;5;241m.\u001b[39mPreTrainedTokenizerBase):\n\u001b[0;32m     65\u001b[0m             pklregister(obj_type)(_save_transformersPreTrainedTokenizerBase)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Unwrap `torch.compile`-ed functions\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'transformers' has no attribute 'PreTrainedTokenizerBase'"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# 2. Dataset Load\n",
    "# ====================================\n",
    "labels = [\"World\",\"Sports\",\"Business\",\"Sci/Tech\"]\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "def select_cols(batch):\n",
    "    texts = []\n",
    "    for i in range(len(batch['label'])):\n",
    "        title = batch.get('title', [None]*len(batch['label']))[i] if 'title' in batch else None\n",
    "        text = batch['text'][i]\n",
    "        headline = title if (title is not None and title != '') else text\n",
    "        texts.append(headline)\n",
    "    return {\"headline\": texts, \"label\": batch[\"label\"]}\n",
    "\n",
    "dataset = dataset.map(select_cols, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7610f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 3. Tokenization\n",
    "# ====================================\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['headline'], truncation=True, max_length=64)\n",
    "\n",
    "encoded = dataset.map(tokenize, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e652ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 4. Model Load\n",
    "# ====================================\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce520cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 5. Metrics\n",
    "# ====================================\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, y = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y, preds),\n",
    "        \"f1\": f1_score(y, preds, average=\"weighted\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 6. Training Arguments\n",
    "# ====================================\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./models\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf624f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 7. Trainer\n",
    "# ====================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded['train'],\n",
    "    eval_dataset=encoded['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 8. Train Model\n",
    "# ====================================\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 9. Evaluate Model\n",
    "# ====================================\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 10. Save Model\n",
    "# ====================================\n",
    "trainer.save_model(\"./models/news_topic_classifier\")\n",
    "tokenizer.save_pretrained(\"./models/news_topic_classifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
